{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b97bf7f-0933-4b56-a1b4-3aab5f4393ca",
   "metadata": {},
   "source": [
    "# 1.Making the data Model in My SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d417c-edd3-4153-ba70-4f94367adf46",
   "metadata": {},
   "source": [
    "CREATE DATABASE corteva_database;      \n",
    "\n",
    "-- Table to store weather data records\n",
    "CREATE TABLE weather_data (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    station_id VARCHAR(50) NOT NULL,           -- The weather_station table (now a VARCHAR)\n",
    "    date DATE NOT NULL,                        -- Date of the weather record (format: YYYY-MM-DD)\n",
    "    max_temp DECIMAL(5,2),                     -- In tenths of a degree Celsius\n",
    "    min_temp DECIMAL(5,2),                     -- In tenths of a degree Celsius\n",
    "    precipitation DECIMAL(6,2),                -- In tenths of a millimeter\n",
    "\n",
    "    UNIQUE KEY unique_date_station (date, station_id)    -- Unique Key combination.\n",
    ");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e99b0-6cd0-420a-9b31-55cbd1e69c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4e5523f-1e39-40e4-96f2-60291e570874",
   "metadata": {},
   "source": [
    "# 2. Data Ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a5a930-d5c0-42e8-8e2a-776f2ccd7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging to store logs in 'weather_data_log.log' file\n",
    "logging.basicConfig(filename='weather_data_log.log',  # Log file path\n",
    "                    filemode='a',  # Append mode to keep existing logs\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Database configuration details, replace with actual credentials\n",
    "# This dictionary holds the necessary parameters to connect to MySQL\n",
    "# Ensure to change these values before deployment to a production environment.\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',  \n",
    "    'password': 'Root@123',\n",
    "    'database': 'corteva_database'\n",
    "}\n",
    "\n",
    "def db_connection(config):\n",
    "    \"\"\"A function to establish MySQL database connection.\"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**config)  # Attempt to establish a connection\n",
    "        return connection  # Return connection object if successful\n",
    "    except mysql.connector.Error as error:\n",
    "        logging.error(f\"Error connecting to the database: {error}\")  # Log the error if connection fails\n",
    "    return None  # Return None in case of failure\n",
    "\n",
    "def files_processing(file_path, connection):\n",
    "    \"\"\"Function to process and insert data from a local file into the MySQL database.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the data file.\n",
    "        connection: Connection object for the MySQL database.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of records inserted into the database.\n",
    "    \"\"\"\n",
    "    total_records = 0  # Counter to track inserted records\n",
    "    station_id = os.path.splitext(os.path.basename(file_path))[0]  # Extract station ID from filename\n",
    "    \n",
    "    with open(file_path, 'r') as data_file:\n",
    "        records_to_insert = []  # List to hold processed records before batch insertion\n",
    "        \n",
    "        for line in data_file:\n",
    "            # Read and parse each line from the file\n",
    "            date, max_temp_tenths, min_temp_tenths, precipitation_tenths = line.strip().split()\n",
    "            \n",
    "            # Convert date format from YYYYMMDD to YYYY-MM-DD\n",
    "            formatted_date = f\"{date[:4]}-{date[4:6]}-{date[6:]}\"\n",
    "            \n",
    "            # Convert values, replacing '-9999' (missing data) with None\n",
    "            max_temp = None if max_temp_tenths == '-9999' else float(max_temp_tenths)  \n",
    "            min_temp = None if min_temp_tenths == '-9999' else float(min_temp_tenths) \n",
    "            precipitation = None if precipitation_tenths == '-9999' else float(precipitation_tenths) / 10  # Convert tenths of mm to cm\n",
    "            \n",
    "            # Append processed record to the list\n",
    "            records_to_insert.append((station_id , formatted_date, max_temp, min_temp, precipitation ))\n",
    "    \n",
    "    if records_to_insert:\n",
    "        cursor = connection.cursor()  # Create a database cursor\n",
    "        batch_insert_query = \"\"\"\n",
    "        INSERT INTO weather_data (station_id , date, max_temp, min_temp, precipitation )\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "        ON DUPLICATE KEY UPDATE \n",
    "            max_temp = VALUES(max_temp),\n",
    "            min_temp = VALUES(min_temp),\n",
    "            precipitation = VALUES(precipitation);\n",
    "        \"\"\"  # Insert new records or update existing ones\n",
    "        try:\n",
    "            cursor.executemany(batch_insert_query, records_to_insert)  # Execute batch insertion\n",
    "            connection.commit()  # Commit changes to the database\n",
    "            total_records += len(records_to_insert)  # Update record count\n",
    "        except mysql.connector.Error as error:\n",
    "            logging.error(f\"Error inserting data from file {file_path}: {error}\")  # Log any insertion error\n",
    "        finally:\n",
    "            cursor.close()  # Close the cursor to release resources\n",
    "    \n",
    "    return total_records  # Return total inserted records\n",
    "\n",
    "def data_ingestion(directory, config):\n",
    "    \"\"\"\n",
    "    Function to ingest data from all files within the local directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing data files.\n",
    "        config: Configuration object for database connection.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conn = db_connection(config)  # Establish database connection\n",
    "    if not conn:\n",
    "        logging.error(\"Failed to connect to the database.\")\n",
    "        return  # Exit if the database connection fails\n",
    "\n",
    "    total_inserted = 0  # Track total inserted records\n",
    "    start_time = datetime.now()  # Capture start time\n",
    "    logging.info(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    logging.info(\"Starting data ingestion process...\")\n",
    "    print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"Starting data ingestion process...\")\n",
    "\n",
    "    # Get a list of all text files in the specified directory\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.txt')]\n",
    "    total_files = len(files)  # Count total files to process\n",
    "\n",
    "    # Display progress using tqdm progress bar\n",
    "    with tqdm(total=total_files, desc=\"Ingesting Data\", unit=\"file\") as pbar:\n",
    "        for filename in files:\n",
    "            full_path = os.path.join(directory, filename)  # Construct full file path\n",
    "            records_inserted = files_processing(full_path, conn)  # Process file and insert data\n",
    "            total_inserted += records_inserted  # Update total inserted count\n",
    "            pbar.update(1)  # Update progress bar\n",
    "\n",
    "    conn.close()  # Close database connection after processing all files\n",
    "    end_time = datetime.now()  # Capture end time\n",
    "    duration = end_time - start_time  # Calculate total processing duration\n",
    "\n",
    "    # Log and print the summary of the ingestion process\n",
    "    logging.info(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    logging.info(f\"Duration: {duration}\")\n",
    "    logging.info(f\"Total records processed: {total_inserted}\")\n",
    "    logging.info(\"Data ingestion completed.\")\n",
    "    print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Duration: {duration}\")\n",
    "    print(f\"Total records processed: {total_inserted}\")\n",
    "    print(\"Data ingestion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f69908af-8417-45e8-92c6-c04a51b8ecd1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-02-06 19:56:44\n",
      "Starting data ingestion process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting Data: 100%|██████████████████████████████████████████████████████████████| 167/167 [02:35<00:00,  1.07file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End time: 2025-02-06 19:59:19\n",
      "Duration: 0:02:35.705451\n",
      "Total records processed: 1729957\n",
      "Data ingestion completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_directory = 'C:/Users/ariza/OneDrive/Desktop/code-challenge-template-main/wx_data'  # local data directory path\n",
    "    data_ingestion(data_directory, db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6a4db6-0511-44a1-a8e8-110350c8b910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf684e-a82c-4359-adca-c29629066ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59f0d6a4-0a84-420d-a10d-c222b7d29d09",
   "metadata": {},
   "source": [
    "# 3. Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a7499-900b-4400-ba71-0a31a278869c",
   "metadata": {},
   "source": [
    "CREATE TABLE weather_statistics (\n",
    "    stat_id INT AUTO_INCREMENT PRIMARY KEY,    -- Stat id to identify the entry\n",
    "    station_id VARCHAR(50) NOT NULL,           -- The weather_station table (now a VARCHAR)\n",
    "    year INT NOT NULL,                         -- Year of the weather record YYYY\n",
    "    avg_max_temp DECIMAL(5, 2),                -- avg_max_temp of the year\n",
    "    avg_min_temp DECIMAL(5, 2),                -- avg_min_temp of the year\n",
    "    total_precipitation DECIMAL(6, 2),         -- total_precipitation of the year\n",
    "    UNIQUE KEY unique_station_year (station_id, year)   -- Unique Key combination.\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950477d-06bf-4e89-9c7d-e2a6ef195187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging similar to the provided data_ingestion example\n",
    "logging.basicConfig(filename='weather_data_log.log',  # Log file path\n",
    "                    filemode='a',  # Append mode\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# Database configuration details, Please replace the host, user, password, database name accordingly.\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',  \n",
    "    'password': 'Root@123',\n",
    "    'database': 'corteva_database'\n",
    "}\n",
    "\n",
    "def db_connection(config):\n",
    "    \"\"\"A function to establish MySQL database connection.\"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**config)\n",
    "        return connection\n",
    "    except mysql.connector.Error as error:\n",
    "        logging.error(f\"Error connecting to the database: {error}\")\n",
    "    return None\n",
    "\n",
    "def weather_data_statistics(config):\n",
    "    \"\"\"\n",
    "    Function to calculate average temperature (minimum and maximum) and total precipitation from weather_data table\n",
    "    and ingest it into a new table in the same database, called weather_statistics.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration object for database connection.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    conn = db_connection(config)  # Assuming db_connection is a function that connects to the database and returns a connection object\n",
    "    if not conn:\n",
    "        logging.error(\"Failed to connect to the database.\")\n",
    "        print(\"Failed to connect to the database.\")\n",
    "        return\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    logging.info(\"Starting weather data statistics calculation...\")\n",
    "    print(\"Starting weather data statistics calculation...\")\n",
    "\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        # Calculate yearly statistics for each station, excluding missing data\n",
    "        query = \"\"\"\n",
    "        SELECT station_id, \n",
    "               YEAR(date) as year, \n",
    "               AVG(max_temp) as avg_max_temp, \n",
    "               AVG(min_temp) as avg_min_temp, \n",
    "               SUM(precipitation) as total_precipitation\n",
    "        FROM weather_data\n",
    "        WHERE max_temp IS NOT NULL AND \n",
    "              min_temp IS NOT NULL AND \n",
    "              precipitation IS NOT NULL\n",
    "        GROUP BY station_id, YEAR(date)\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Process and insert calculated statistics\n",
    "        for row in tqdm(results, desc=\"Updating statistics\"):\n",
    "            station_id, year, avg_max_temp, avg_min_temp, total_precipitation = row\n",
    "            upsert_query = \"\"\"\n",
    "            INSERT INTO weather_statistics (station_id, year, avg_max_temp, avg_min_temp, total_precipitation)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            ON DUPLICATE KEY UPDATE avg_max_temp=VALUES(avg_max_temp), \n",
    "                                    avg_min_temp=VALUES(avg_min_temp),\n",
    "                                    total_precipitation=VALUES(total_precipitation)\n",
    "            \"\"\"\n",
    "            cursor.execute(upsert_query, (station_id, year, avg_max_temp, avg_min_temp, total_precipitation))\n",
    "        conn.commit()\n",
    "    except Error as e:\n",
    "        logging.error(f\"Database error: {e}\")\n",
    "    finally:\n",
    "        if conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            logging.info(\"Database connection closed.\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    logging.info(f\"Statistics calculation completed. Start time: {start_time}, End time: {end_time}, Duration: {duration}\")\n",
    "    print(f\"Statistics calculation completed. Start time: {start_time}, End time: {end_time}, Duration: {duration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0728f408-cf25-4aea-9df0-2476bab457e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weather data statistics calculation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating statistics: 100%|███████████████████████████████████████████████████████| 4791/4791 [00:02<00:00, 2395.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics calculation completed. Start time: 2025-02-06 20:05:08.232571, End time: 2025-02-06 20:05:33.032399, Duration: 0:00:24.799828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    weather_data_statistics(db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ce9a0-e301-46bc-9743-fac801cd33e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15c7963b-4fc7-4751-9632-70ab4e15959a",
   "metadata": {},
   "source": [
    "# 4. API creation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328c449-ba89-4f39-a209-9dd07e484cbf",
   "metadata": {},
   "source": [
    "Step : \n",
    "1. run : python API_CALL.py \n",
    "2. URL : http://127.0.0.1:5000/api/weather\n",
    "-> http://127.0.0.1:5000/api/weather/stats\n",
    "Screenshot Attached for the Json output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9464e-2118-486a-ab75-f9e3e816aabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4104cd2-2320-4bcd-9398-6808c41aa7bb",
   "metadata": {},
   "source": [
    "# EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464d8ed-e1b7-4bbf-aa25-32303fd69420",
   "metadata": {},
   "source": [
    "Deploying a complete weather data analysis and API service on AWS involves multiple stages, including database setup, data ingestion, data processing, and exposing the processed data through an API.\n",
    "\n",
    "Task 1: Data Modeling\n",
    "For storing relational data on AWS, Amazon RDS with MySQL is a strong choice, as it supports familiar SQL-based data modeling. The database schema remains consistent with previously mentioned structures, making it well-suited for MySQL on Amazon RDS.\n",
    "\n",
    "Task 2: Data Ingestion\n",
    "Raw weather data can be initially stored in Amazon S3. To process this data, an AWS Lambda function is utilized. This serverless compute service triggers code execution when data changes occur in an S3 bucket, making it ideal for ingestion tasks. The Lambda function reads raw data files from S3, applies necessary transformations, and loads the processed data into the Amazon RDS MySQL database.\n",
    "\n",
    "Task 3: Data Analysis\n",
    "AWS Lambda works alongside Amazon RDS to compute and store statistical insights. A Lambda function can be scheduled to run SQL queries that calculate key metrics, such as the average minimum and maximum temperatures and total precipitation per station annually. These computed results are then stored in a dedicated table within the RDS database for easy retrieval and analysis.\n",
    "\n",
    "Task 4: REST API\n",
    "To make both raw and analyzed weather data accessible, a REST API is developed using Flask. AWS Elastic Beanstalk, a Platform-as-a-Service (PaaS) offering, simplifies the deployment and scaling of the API. By hosting the Flask application on Elastic Beanstalk, API endpoints such as /api/weather and /api/weather/stats become available. These endpoints return JSON-formatted weather data, supporting filtering by station ID and date, with responses served directly from the RDS database.\n",
    "\n",
    "AWS Services Utilized\n",
    "Amazon RDS – Managed relational database for structured weather data storage.\n",
    "Amazon S3 – Secure, scalable storage for raw weather data.\n",
    "AWS Lambda – Serverless compute service for automating data ingestion and analysis.\n",
    "AWS Elastic Beanstalk – PaaS for easy API deployment and scaling.\n",
    "By leveraging these AWS services, this solution ensures a scalable, efficient, and well-integrated weather data processing system, from ingestion to analysis and API accessibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425ab3e-8ef9-40cb-9e6f-9803177da33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
